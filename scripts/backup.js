#!/usr/bin/env node

const { Client } = require("pg");
const fs = require("fs");
const path = require("path");
const zlib = require("zlib");

/**
 * Professional PostgreSQL Backup Script
 * Supports all database objects: tables, views, sequences, functions, triggers, enums, etc.
 * Serverless-compatible (no pg_dump required)
 */

async function createBackup(connectionString, backupName, options = {}, progressCallback = null) {
  const {
    compress = false,
    schemasOnly = null,
    dataOnly = false,
    schemaOnly = false,
  } = options;

  const client = new Client({
    connectionString: connectionString,
    ssl: {
      rejectUnauthorized: false,
    },
  });

  // Helper to emit progress events
  const emit = (event) => {
    if (progressCallback) {
      progressCallback(event);
    } else {
      // Fallback to console.log for CLI usage
      if (event.message) {
        console.log(`${event.icon || ''} ${event.message}`);
      }
    }
  };

  // Progress tracking
  let totalSteps = 0;
  let completedSteps = 0;

  const updateProgress = () => {
    const progress = totalSteps > 0 ? Math.round((completedSteps / totalSteps) * 100) : 0;
    return progress;
  };

  try {
    emit({ type: 'log', stage: 'connecting', icon: 'üîå', message: 'Connecting to database...', logType: 'info', progress: 0 });
    await client.connect();
    emit({ type: 'log', stage: 'connecting', icon: '‚úÖ', message: 'Connected successfully!', logType: 'success', progress: 5 });

    // Get database info
    emit({ type: 'log', stage: 'info', icon: '‚ÑπÔ∏è', message: 'Fetching database information...', logType: 'info', progress: 10 });
    const dbInfo = await client.query("SELECT current_database(), version()");
    const databaseName = dbInfo.rows[0].current_database;
    const postgresVersion = dbInfo.rows[0].version;

    const backupDir = path.join(process.cwd(), "backups");
    if (!fs.existsSync(backupDir)) {
      fs.mkdirSync(backupDir, { recursive: true });
    }

    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const filename = `${backupName}_${timestamp}.sql${compress ? ".gz" : ""}`;
    const filepath = path.join(backupDir, filename);
    const metadataPath = path.join(
      backupDir,
      `${backupName}_${timestamp}.json`,
    );

    emit({ type: 'log', stage: 'info', icon: 'üìä', message: `Database: ${databaseName}`, logType: 'info', progress: 12 });

    let writeStream;
    let sqlContent = "";

    const write = (content) => {
      sqlContent += content;
    };

    // Write header
    write(`-- PostgreSQL Database Backup\n`);
    write(`-- Backup Name: ${backupName}\n`);
    write(`-- Database: ${databaseName}\n`);
    write(`-- PostgreSQL Version: ${postgresVersion}\n`);
    write(`-- Created: ${new Date().toISOString()}\n`);
    write(`-- Generated by: Professional PostgreSQL Backup Tool\n\n`);
    write(`SET statement_timeout = 0;\n`);
    write(`SET lock_timeout = 0;\n`);
    write(`SET client_encoding = 'UTF8';\n`);
    write(`SET standard_conforming_strings = on;\n`);
    write(`SET check_function_bodies = false;\n`);
    write(`SET xmloption = content;\n`);
    write(`SET client_min_messages = warning;\n\n`);

    // Get all schemas (excluding system schemas)
    emit({ type: 'log', stage: 'schemas', icon: 'üìã', message: 'Fetching schemas...', logType: 'info', progress: 15 });
    const schemasResult = await client.query(
      `
      SELECT schema_name
      FROM information_schema.schemata
      WHERE schema_name NOT IN ('pg_catalog', 'information_schema', 'pg_toast', 'pg_temp_1', 'pg_toast_temp_1')
      ${schemasOnly ? `AND schema_name = ANY($1)` : ""}
      ORDER BY schema_name;
    `,
      schemasOnly ? [schemasOnly] : [],
    );

    const schemas = schemasResult.rows.map((r) => r.schema_name);
    emit({ type: 'log', stage: 'schemas', icon: '‚úÖ', message: `Found ${schemas.length} schema(s): ${schemas.join(', ')}`, logType: 'success', progress: 18 });

    const objectCounts = {
      tables: 0,
      views: 0,
      sequences: 0,
      functions: 0,
      triggers: 0,
      enums: 0,
      extensions: 0,
    };

    // Backup extensions
    if (!dataOnly) {
      emit({ type: 'log', stage: 'extensions', icon: 'üîå', message: 'Backing up extensions...', logType: 'info', progress: 20 });
      const extensionsResult = await client.query(`
        SELECT extname, extversion
        FROM pg_extension
        WHERE extname NOT IN ('plpgsql')
        ORDER BY extname;
      `);

      if (extensionsResult.rows.length > 0) {
        write(`\n-- Extensions\n`);
        for (const ext of extensionsResult.rows) {
          write(
            `CREATE EXTENSION IF NOT EXISTS "${ext.extname}" WITH SCHEMA public VERSION '${ext.extversion}';\n`,
          );
          objectCounts.extensions++;
          emit({ type: 'log', stage: 'extensions', icon: '‚úÖ', message: `Extension: ${ext.extname} v${ext.extversion}`, logType: 'info', progress: 20 + objectCounts.extensions });
        }
        write(`\n`);
      }
    }

    // Backup enums and custom types
    if (!dataOnly) {
      emit({ type: 'log', stage: 'enums', icon: 'üè∑Ô∏è', message: 'Backing up enums and custom types...', logType: 'info', progress: 25 });
      for (const schema of schemas) {
        const enumsResult = await client.query(
          `
          SELECT t.typname, e.enumlabel
          FROM pg_type t
          JOIN pg_enum e ON t.oid = e.enumtypid
          JOIN pg_namespace n ON t.typnamespace = n.oid
          WHERE n.nspname = $1
          ORDER BY t.typname, e.enumsortorder;
        `,
          [schema],
        );

        if (enumsResult.rows.length > 0) {
          const enumsByType = {};
          for (const row of enumsResult.rows) {
            if (!enumsByType[row.typname]) {
              enumsByType[row.typname] = [];
            }
            enumsByType[row.typname].push(row.enumlabel);
          }

          write(`\n-- Enums in schema: ${schema}\n`);
          for (const [typname, labels] of Object.entries(enumsByType)) {
            write(`DROP TYPE IF EXISTS ${schema}.${typname} CASCADE;\n`);
            write(`CREATE TYPE ${schema}.${typname} AS ENUM (\n`);
            write(
              labels.map((l) => `  '${l.replace(/'/g, "''")}'`).join(",\n"),
            );
            write(`\n);\n\n`);
            objectCounts.enums++;
            emit({ type: 'log', stage: 'enums', icon: '‚úÖ', message: `Enum: ${schema}.${typname}`, logType: 'info', progress: 25 });
          }
        }
      }
    }

    // Count total tables for progress calculation
    let totalTables = 0;
    let processedTables = 0;
    for (const schema of schemas) {
      const countResult = await client.query(
        `SELECT COUNT(*) as count FROM information_schema.tables WHERE table_schema = $1 AND table_type = 'BASE TABLE'`,
        [schema]
      );
      totalTables += parseInt(countResult.rows[0].count);
    }

    emit({ type: 'log', stage: 'tables', icon: 'üìä', message: `Found ${totalTables} table(s) to backup`, logType: 'info', progress: 30 });

    for (const schema of schemas) {
      write(`\n-- ============================================\n`);
      write(`-- Schema: ${schema}\n`);
      write(`-- ============================================\n\n`);
      write(`CREATE SCHEMA IF NOT EXISTS ${schema};\n\n`);

      // Backup sequences
      if (!dataOnly) {
        emit({ type: 'log', stage: 'sequences', icon: 'üî¢', message: `Backing up sequences from schema: ${schema}...`, logType: 'info', progress: 30 });
        const sequencesResult = await client.query(
          `
          SELECT sequence_name
          FROM information_schema.sequences
          WHERE sequence_schema = $1
          ORDER BY sequence_name;
        `,
          [schema],
        );

        for (const seqRow of sequencesResult.rows) {
          const seqName = seqRow.sequence_name;
          const fullSeqName = `${schema}.${seqName}`;

          const seqDetails = await client.query(`
            SELECT * FROM ${fullSeqName}
          `);

          if (seqDetails.rows.length > 0) {
            const seq = seqDetails.rows[0];
            write(`\n-- Sequence: ${fullSeqName}\n`);
            write(`DROP SEQUENCE IF EXISTS ${fullSeqName} CASCADE;\n`);
            write(`CREATE SEQUENCE ${fullSeqName}\n`);
            write(`  START WITH ${seq.last_value}\n`);
            write(`  INCREMENT BY ${seq.increment_by || 1}\n`);
            write(`  MINVALUE ${seq.min_value}\n`);
            write(`  MAXVALUE ${seq.max_value}\n`);
            write(`  CACHE ${seq.cache_size || 1};\n\n`);
            objectCounts.sequences++;
          }
        }
      }

      // Backup views
      if (!dataOnly) {
        emit({ type: 'log', stage: 'views', icon: 'üëÅÔ∏è', message: `Backing up views from schema: ${schema}...`, logType: 'info', progress: 35 });
        const viewsResult = await client.query(
          `
          SELECT table_name, view_definition
          FROM information_schema.views
          WHERE table_schema = $1
          ORDER BY table_name;
        `,
          [schema],
        );

        for (const viewRow of viewsResult.rows) {
          const viewName = viewRow.table_name;
          const fullViewName = `${schema}.${viewName}`;
          write(`\n-- View: ${fullViewName}\n`);
          write(`DROP VIEW IF EXISTS ${fullViewName} CASCADE;\n`);
          write(
            `CREATE VIEW ${fullViewName} AS\n${viewRow.view_definition}\n\n`,
          );
          objectCounts.views++;
        }
      }

      // Get all tables in this schema
      emit({ type: 'log', stage: 'tables', icon: 'üìä', message: `Fetching tables from schema: ${schema}...`, logType: 'info', progress: 40 });
      const tablesResult = await client.query(
        `
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = $1 AND table_type = 'BASE TABLE'
        ORDER BY table_name;
      `,
        [schema],
      );

      for (const tableRow of tablesResult.rows) {
        const table = tableRow.table_name;
        const fullTableName = `${schema}.${table}`;

        // Calculate progress based on tables processed
        const tableProgress = 40 + Math.round((processedTables / totalTables) * 40);
        emit({
          type: 'log',
          stage: 'tables',
          icon: 'üì¶',
          message: `Backing up table: ${fullTableName}...`,
          logType: 'info',
          progress: tableProgress,
          currentTable: fullTableName,
          metadata: { tablesProcessed: processedTables, totalTables }
        });

        if (!dataOnly) {
          // Get table structure
          const columnsResult = await client.query(
            `
            SELECT 
              column_name,
              data_type,
              udt_name,
              character_maximum_length,
              numeric_precision,
              numeric_scale,
              column_default,
              is_nullable,
              is_identity,
              identity_generation
            FROM information_schema.columns
            WHERE table_schema = $1 AND table_name = $2
            ORDER BY ordinal_position;
          `,
            [schema, table],
          );

          write(`\n-- Table: ${fullTableName}\n`);
          write(`DROP TABLE IF EXISTS ${fullTableName} CASCADE;\n`);
          write(`CREATE TABLE ${fullTableName} (\n`);

          const columnDefs = columnsResult.rows.map((col, idx) => {
            let def = `  "${col.column_name}" `;

            // Handle data type
            if (col.data_type === "ARRAY") {
              def += `${col.udt_name.replace("_", "")}[]`;
            } else if (col.data_type === "USER-DEFINED") {
              def += col.udt_name;
            } else {
              def += col.data_type.toUpperCase();
              if (col.character_maximum_length) {
                def += `(${col.character_maximum_length})`;
              } else if (col.numeric_precision) {
                def += `(${col.numeric_precision}${col.numeric_scale ? "," + col.numeric_scale : ""})`;
              }
            }

            // Handle identity columns
            if (col.is_identity === "YES") {
              def += ` GENERATED ${col.identity_generation} AS IDENTITY`;
            } else if (col.column_default) {
              def += ` DEFAULT ${col.column_default}`;
            }

            if (col.is_nullable === "NO") {
              def += " NOT NULL";
            }

            return def;
          });

          write(columnDefs.join(",\n"));
          write("\n);\n\n");

          // Get primary keys
          const pkResult = await client.query(
            `
            SELECT a.attname
            FROM pg_index i
            JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
            WHERE i.indrelid = $1::regclass AND i.indisprimary;
          `,
            [fullTableName],
          );

          if (pkResult.rows.length > 0) {
            const pkColumns = pkResult.rows
              .map((r) => `"${r.attname}"`)
              .join(", ");
            write(
              `ALTER TABLE ${fullTableName} ADD PRIMARY KEY (${pkColumns});\n\n`,
            );
          }

          // Get unique constraints
          const uniqueResult = await client.query(
            `
            SELECT conname, pg_get_constraintdef(oid) as condef
            FROM pg_constraint
            WHERE conrelid = $1::regclass AND contype = 'u';
          `,
            [fullTableName],
          );

          for (const uc of uniqueResult.rows) {
            write(
              `ALTER TABLE ${fullTableName} ADD CONSTRAINT "${uc.conname}" ${uc.condef};\n`,
            );
          }

          // Get check constraints
          const checkResult = await client.query(
            `
            SELECT conname, pg_get_constraintdef(oid) as condef
            FROM pg_constraint
            WHERE conrelid = $1::regclass AND contype = 'c';
          `,
            [fullTableName],
          );

          for (const cc of checkResult.rows) {
            write(
              `ALTER TABLE ${fullTableName} ADD CONSTRAINT "${cc.conname}" ${cc.condef};\n`,
            );
          }

          if (uniqueResult.rows.length > 0 || checkResult.rows.length > 0) {
            write("\n");
          }

          objectCounts.tables++;
        }

        // Get data
        if (!schemaOnly) {
          const countResult = await client.query(
            `SELECT COUNT(*) as count FROM ${fullTableName}`,
          );
          const rowCount = parseInt(countResult.rows[0].count);

          if (rowCount > 0) {
            write(`-- Data for ${fullTableName} (${rowCount} rows)\n`);
            emit({
              type: 'log',
              stage: 'data',
              icon: 'üíæ',
              message: `Exporting ${rowCount} row(s) from ${fullTableName}`,
              logType: 'info',
              progress: tableProgress,
              currentTable: fullTableName,
              rowsProcessed: rowCount
            });

            const dataResult = await client.query(
              `SELECT * FROM ${fullTableName}`,
            );
            const columns = dataResult.fields.map((f) => f.name);

            for (const row of dataResult.rows) {
              const values = columns.map((col) => {
                const val = row[col];
                if (val === null) return "NULL";
                if (typeof val === "boolean") return val ? "true" : "false";
                if (typeof val === "number") return val.toString();
                if (val instanceof Date) return `'${val.toISOString()}'`;
                if (typeof val === "object") {
                  return `'${JSON.stringify(val).replace(/'/g, "''")}'`;
                }
                if (typeof val === "string") {
                  return `'${val.replace(/'/g, "''").replace(/\\/g, "\\\\")}'`;
                }
                return `'${String(val).replace(/'/g, "''")}'`;
              });

              write(
                `INSERT INTO ${fullTableName} (${columns.map((c) => `"${c}"`).join(", ")}) VALUES (${values.join(", ")});\n`,
              );
            }
            write("\n");
          }
        }

        processedTables++;
        emit({
          type: 'log',
          stage: 'tables',
          icon: '‚úÖ',
          message: `Completed table: ${fullTableName}`,
          logType: 'success',
          progress: 40 + Math.round((processedTables / totalTables) * 40),
          currentTable: fullTableName
        });
      }

      // Backup indexes (excluding primary key indexes)
      if (!dataOnly) {
        emit({ type: 'log', stage: 'indexes', icon: 'üîç', message: `Backing up indexes from schema: ${schema}...`, logType: 'info', progress: 80 });
        const indexesResult = await client.query(
          `
          SELECT indexname, indexdef
          FROM pg_indexes
          WHERE schemaname = $1
          AND indexname NOT LIKE '%_pkey'
          ORDER BY tablename, indexname;
        `,
          [schema],
        );

        if (indexesResult.rows.length > 0) {
          write(`\n-- Indexes for schema: ${schema}\n`);
          for (const idx of indexesResult.rows) {
            write(`${idx.indexdef};\n`);
          }
          write("\n");
        }
      }

      // Backup foreign keys
      if (!dataOnly) {
        emit({ type: 'log', stage: 'foreignkeys', icon: 'üîó', message: `Backing up foreign keys from schema: ${schema}...`, logType: 'info', progress: 85 });
        const fkResult = await client.query(
          `
          SELECT
            tc.table_name,
            tc.constraint_name,
            pg_get_constraintdef(pgc.oid) as condef
          FROM information_schema.table_constraints tc
          JOIN pg_constraint pgc ON pgc.conname = tc.constraint_name
          JOIN pg_namespace nsp ON nsp.oid = pgc.connamespace
          WHERE tc.constraint_type = 'FOREIGN KEY'
          AND tc.table_schema = $1
          ORDER BY tc.table_name, tc.constraint_name;
        `,
          [schema],
        );

        if (fkResult.rows.length > 0) {
          write(`\n-- Foreign Keys for schema: ${schema}\n`);
          for (const fk of fkResult.rows) {
            write(
              `ALTER TABLE ${schema}.${fk.table_name} ADD CONSTRAINT "${fk.constraint_name}" ${fk.condef};\n`,
            );
          }
          write("\n");
        }
      }

      // Backup functions and procedures
      if (!dataOnly) {
        emit({ type: 'log', stage: 'functions', icon: '‚öôÔ∏è', message: `Backing up functions from schema: ${schema}...`, logType: 'info', progress: 88 });
        const functionsResult = await client.query(
          `
          SELECT
            p.proname as function_name,
            pg_get_functiondef(p.oid) as function_def
          FROM pg_proc p
          JOIN pg_namespace n ON p.pronamespace = n.oid
          WHERE n.nspname = $1
          AND p.prokind IN ('f', 'p')
          ORDER BY p.proname;
        `,
          [schema],
        );

        if (functionsResult.rows.length > 0) {
          write(`\n-- Functions and Procedures for schema: ${schema}\n`);
          for (const func of functionsResult.rows) {
            write(`${func.function_def};\n\n`);
            objectCounts.functions++;
          }
        }
      }

      // Backup triggers
      if (!dataOnly) {
        emit({ type: 'log', stage: 'triggers', icon: '‚ö°', message: `Backing up triggers from schema: ${schema}...`, logType: 'info', progress: 90 });
        const triggersResult = await client.query(
          `
          SELECT
            tgname as trigger_name,
            tgrelid::regclass as table_name,
            pg_get_triggerdef(oid) as trigger_def
          FROM pg_trigger
          WHERE tgisinternal = false
          AND tgrelid::regclass::text LIKE $1 || '.%'
          ORDER BY tgname;
        `,
          [schema],
        );

        if (triggersResult.rows.length > 0) {
          write(`\n-- Triggers for schema: ${schema}\n`);
          for (const trig of triggersResult.rows) {
            write(`${trig.trigger_def};\n`);
            objectCounts.triggers++;
          }
          write("\n");
        }
      }
    }

    // Write the final SQL content to file
    emit({ type: 'log', stage: 'writing', icon: 'üíæ', message: compress ? 'Compressing and writing backup file...' : 'Writing backup file...', logType: 'info', progress: 92 });
    if (compress) {
      const compressed = zlib.gzipSync(sqlContent);
      fs.writeFileSync(filepath, compressed);
    } else {
      fs.writeFileSync(filepath, sqlContent);
    }

    const fileSize = fs.statSync(filepath).size;
    emit({ type: 'log', stage: 'writing', icon: '‚úÖ', message: `File written: ${(fileSize / 1024).toFixed(2)} KB`, logType: 'success', progress: 95 });

    // Create metadata file
    const metadata = {
      backupName,
      timestamp: new Date().toISOString(),
      databaseName,
      postgresVersion,
      fileSize,
      compressed: compress,
      schemas,
      objectCounts,
    };

    fs.writeFileSync(metadataPath, JSON.stringify(metadata, null, 2));
    emit({ type: 'log', stage: 'metadata', icon: 'üìã', message: 'Metadata file created', logType: 'success', progress: 98 });

    emit({ type: 'log', stage: 'complete', icon: '‚úÖ', message: `Backup completed successfully!`, logType: 'success', progress: 100 });
    emit({ type: 'log', stage: 'complete', icon: 'üìÅ', message: `File: ${filename}`, logType: 'info', progress: 100 });
    emit({ type: 'log', stage: 'complete', icon: 'üìä', message: `Size: ${(fileSize / 1024).toFixed(2)} KB`, logType: 'info', progress: 100 });
    emit({ type: 'log', stage: 'complete', icon: 'üìà', message: `Tables: ${objectCounts.tables} | Views: ${objectCounts.views} | Functions: ${objectCounts.functions}`, logType: 'info', progress: 100 });

    emit({
      type: 'complete',
      stage: 'complete',
      progress: 100,
      result: { filepath, metadata, filename, fileSize, objectCounts }
    });

    return { filepath, metadata };
  } catch (error) {
    emit({ type: 'error', stage: 'error', icon: '‚ùå', message: `Backup failed: ${error.message}`, logType: 'error', progress: 0 });
    throw error;
  } finally {
    await client.end();
  }
}

// CLI usage
if (require.main === module) {
  const args = process.argv.slice(2);

  if (args.length < 2) {
    console.log(
      "Usage: node backup.js <connection_string> <backup_name> [--compress]",
    );
    console.log(
      'Example: node backup.js "postgresql://user:pass@host/db" my_backup --compress',
    );
    process.exit(1);
  }

  const [connectionString, backupName, ...flags] = args;
  const options = {
    compress: flags.includes("--compress"),
  };

  createBackup(connectionString, backupName, options)
    .then(() => process.exit(0))
    .catch(() => process.exit(1));
}

module.exports = { createBackup };
